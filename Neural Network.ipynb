{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb8cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establishing number of nodes in an array\n",
    "n = [2,3,3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba8d049-24d2-48fc-a6cb-210fd18d03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Neural Network Layer Weights. If l is the current indexed layer, then the dimensions of the weight matrix should be n^[l]xn^[(l-1)] where n[l]\n",
    "#are the features in that layer\n",
    "W1 = np.random.randn(n[1], n[0])\n",
    "W2 = np.random.randn(n[2], n[1])\n",
    "W3 = np.random.randn(n[3], n[2])\n",
    "#This creates a 2 dimensional array\n",
    "\n",
    "#Neural Network bias weights. Biases are just an n[l] x 1 matrix since each node only has 1 bias.\n",
    "b1 = np.random.randn(n[1], 1)\n",
    "b2 = np.random.randn(n[2], 1)\n",
    "b3 = np.random.randn(n[3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2973da64-1bcc-4862-9871-7f9d941f9399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for layer 1 shape: (3, 2)\n",
      "Weights for layer 2 shape: (3, 3)\n",
      "Weights for layer 3 shape: (1, 3)\n",
      "bias for layer 1 shape: (3, 1)\n",
      "bias for layer 2 shape: (3, 1)\n",
      "bias for layer 3 shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#print the values as a test\n",
    "\n",
    "print(\"Weights for layer 1 shape:\", W1.shape)\n",
    "print(\"Weights for layer 2 shape:\", W2.shape)\n",
    "print(\"Weights for layer 3 shape:\", W3.shape)\n",
    "print(\"bias for layer 1 shape:\", b1.shape)\n",
    "print(\"bias for layer 2 shape:\", b2.shape)\n",
    "print(\"bias for layer 3 shape:\", b3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529d87e1-7965-432a-86cb-50ab8db00cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "#Neural Network Training Data - Input\n",
    "X = np.array([\n",
    "    [150, 70],\n",
    "    [254, 73],\n",
    "    [312, 68],\n",
    "    [120, 60],\n",
    "    [154, 61],\n",
    "    [212, 65],\n",
    "    [216, 67],\n",
    "    [145, 67],\n",
    "    [184, 64],\n",
    "    [130, 69]\n",
    "])\n",
    "print(X.shape)\n",
    "\n",
    "#Vectorization\n",
    "A0 = X.T\n",
    "\n",
    "#confirm transpose\n",
    "print(A0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df0736b-5ccd-4acc-8859-5a93c06b4a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network - Labels\n",
    "y =  np.array([\n",
    "    0,  \n",
    "    1,   \n",
    "    1, \n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "m = 10\n",
    "\n",
    "#Labels must be in the form n[3]^m since there is only 1 binary result of cardiovacular disease for each piece of training data\n",
    "Y = y.reshape(n[3], m)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756e6ab6-90e2-4331-9d84-796d9cc54f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#activation function\n",
    "def sigmoid(arr):\n",
    "    return 1 / (1 + np.exp(-1 * arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e894b4bf-fb59-4605-ac90-b2411ff52268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a forward propogation function\n",
    "#reversed weights and biases as parameters (lists that start with W1 and b1 instead of W3 and b3)\n",
    "def forward_prop(layers, Weights_rev, biases_rev, A):\n",
    "    \"\"\"\n",
    "    Rakshit's Notes:\n",
    "    Note here that I add b which is a vector to a matrix. This is not mathematically defined but in Numpy we can \"broadcast\" the column values\n",
    "     of b so that each entry of the b vector is repeatedly applied to the columns of the W @ A matrix\n",
    "     \n",
    "     You can read more by checking out this stack overflow question - https://stackoverflow.com/questions/15744402/numpy-matrix-plus-column-vector\n",
    "     And the Numpy docs - https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "    \"\"\"\n",
    "    #Activated outputs of each layer\n",
    "    activated_out = []\n",
    "    #Let's make this function scalable to l layers\n",
    "    for l in layers:\n",
    "        Z =  Weight_rev[l] @ A0 + biases_rev[l]\n",
    "        A = sigmoid(Z)\n",
    "        #Add layer output to output array\n",
    "        activated_out.append(A)\n",
    "    #reverse the array for future backprop calculations\n",
    "    activated_out.reverse()    \n",
    "    return activated_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb41da62-5fcc-42b8-95e3-dced20cd1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAN IGNORE THE CODE BELOW - Testing code for unscaled forward_prop\n",
    "A1 = forward_prop(W1, b1, A0)\n",
    "#initial check for function working as intended\n",
    "assert A1.shape == (n[1], m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e6ccec-6c70-40fc-9614-44bbb3af35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CAN IGNORE THE CODE BELOW - Testing code for unscaled forward_prop\n",
    "A2 = forward_prop(W2, b2, A1)\n",
    "A3 = forward_prop(W3, b3, A2)\n",
    "y_hat = A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5fa4404-74e9-4e3f-9a10-7c8d85f38546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat, y):\n",
    "  \"\"\"\n",
    "  y_hat should be a n^L x m matrix\n",
    "  y should be a n^L x m matrix\n",
    "  \"\"\"\n",
    "  \n",
    "  \"\"\"\n",
    "  Rakshit's Notes:\n",
    "  Bernoulli distribution based \"binary cross-entropy loss\" calculation. Essentially we use this since we use sigmoid (which outputs the probability of cardiovascular disease) as our \n",
    "  activation function and because the Neural Network returns a true or false answer, just like a bernoulli trial.\n",
    "  \n",
    "  Therefore, we can reduce the cost by maximizing the negative of the cost function. (ECE 204 application lol)\n",
    "  \n",
    "  Also note that this is the log of the intuitive Bernoulli PDF. This is done so that we only have to deal with a summation rather than a multiplication\n",
    "  of the test samples. Since y_hat can only take values between 0 and 1 (as it is a probability), the below loss function has a minimum on this closed interval. \n",
    "  \"\"\"\n",
    " # 1. losses is a n^L x m\n",
    "  losses = - ((y * np.log(y_hat)) + (1 - y)*np.log(1 - y_hat))\n",
    "\n",
    "  \"\"\"\n",
    "  Rakshit's Notes:\n",
    "  flattens multi-dimensional numpy array into a 1D shape array. Therefore, shape[0]\n",
    "  returns only the ammount of elements in the array.\n",
    "  \n",
    "  This is a neat trick for when the sample size is not known or if it increases later during training.\n",
    "  \"\"\"\n",
    "  m = y_hat.reshape(-1).shape[0]\n",
    "\n",
    "  # 2. summing across axis = 1 means we sum across rows, \n",
    "  #   making this a n^L x 1 matrix\n",
    "  summed_losses = (1 / m) * np.sum(losses, axis=1)\n",
    "\n",
    "  # 3. unnecessary, but useful if working with more than one node\n",
    "  #   in output layer\n",
    "  return np.sum(summed_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c690ccb5-8591-4e2b-aa78-fdcef5dbc66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.695108249978964"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(y_hat,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59158da-0391-4949-a4c6-1ebc1f029818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rakshit's notes - Deviating here from Aadil's code. \n",
    "# Rakshit's notes - I want to make this function scalable in case I want to add more hidden layers.\n",
    "weights = [W3,W2,W1]\n",
    "biases = [b3, b2, b1]\n",
    "activated_out = [A3, A2, A1]\n",
    "\n",
    "\"\"\"\n",
    "I don't provide any explanation of the math used in back_prop as comments below.\n",
    "If you are interested in learning the math and intuition of backwards propagation, please see the README file on my github at https://github.com/Rak256/Neural-Network-/blob/main/README.md\n",
    "I explain all the mathematical research I have done for this project there along with the sources I used.\n",
    "\"\"\"\n",
    "def back_prop(layers, activated_out, weights):\n",
    "    dZ = (1/m)*(activated_out[0] - Y)\n",
    "    dW_array = []\n",
    "    db_array = []\n",
    "    for l in range(layers):\n",
    "        #activated out is accessed by the l + 1 index since activated_out[0] is the network output which is already accounted for above (outside the for loop).\n",
    "        dW = dZ @ activated_out[l + 1].T\n",
    "        \n",
    "        #must keep dimensions to avoid broadcasting of the bias matrix in each iteration of back prop.\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)\n",
    "        \n",
    "        dA = (weights[l].T) @ dZ\n",
    "        dZ = dA*activated_out[l + 1]*(1 - activated_out[l + 1])\n",
    "        dW_array.append(dW)\n",
    "        db_array.append(db)\n",
    "    return dW_array, db_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a8d1a6e2-1103-4ef9-aa17-36ab4e59d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Rakshit's Notes:\n",
    "Final setup - Putting everything together.\n",
    "Here I create a train function similar to Aadil's Medium article\n",
    "all the comments below are mine\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "#These variables would have to be used with the test function and the main program function\n",
    "# So, it's better to make them global\n",
    "\n",
    "n = [2,3,3,1]\n",
    "\n",
    "X = np.array([\n",
    "    [150, 70],\n",
    "    [254, 73],\n",
    "    [312, 68],\n",
    "    [120, 60],\n",
    "    [154, 61],\n",
    "    [212, 65],\n",
    "    [216, 67],\n",
    "    [145, 67],\n",
    "    [184, 64],\n",
    "    [130, 69]\n",
    "])\n",
    "\n",
    "#Vectorization\n",
    "A0 = X.T\n",
    "\n",
    "y =  np.array([\n",
    "    0,  \n",
    "    1,   \n",
    "    1, \n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "m = 10\n",
    "\n",
    "Y = y.reshape(n[3], m)\n",
    "\n",
    "weights = [] \n",
    "biases = []\n",
    "\n",
    "def sigmoid(arr):\n",
    "    return 1 / (1 + np.exp(-1 * arr))\n",
    "\n",
    "def forward_prop(layers, Weights_rev, biases_rev):\n",
    "    A = A0\n",
    "    activated_out = [A]\n",
    "    for l in range(layers):\n",
    "        Z =  Weights_rev[l] @ A + biases_rev[l]\n",
    "        A = sigmoid(Z)\n",
    "        activated_out.append(A)\n",
    "        \n",
    "    activated_out.reverse()    \n",
    "    return activated_out\n",
    "\n",
    "def cost(y_hat, y):\n",
    "  losses = - ((y * np.log(y_hat)) + (1 - y)*np.log(1 - y_hat))\n",
    "\n",
    "  m = y_hat.reshape(-1).shape[0]\n",
    "  summed_losses = (1 / m) * np.sum(losses, axis=1)\n",
    "  return np.sum(summed_losses)\n",
    "\n",
    "\n",
    "def back_prop(layers, activated_out, weights):\n",
    "    dZ = (1/m)*(activated_out[0] - Y)\n",
    "    dW_array = []\n",
    "    db_array = []\n",
    "    for l in range(layers):\n",
    "        dW = dZ @ activated_out[l + 1].T\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = (weights[l].T) @ dZ\n",
    "        dZ = dA*activated_out[l + 1]*(1 - activated_out[l + 1])\n",
    "        dW_array.append(dW)\n",
    "        db_array.append(db)\n",
    "    return dW_array, db_array\n",
    "\n",
    "def train(A0):\n",
    "    layers = 3\n",
    "    \n",
    "    \"\"\"\n",
    "    Goal: To create weight matrix and bias matrix, indexed starting from the\n",
    "    outer layers of the network. So we must index the array n from its end to its beginning index, 0.\n",
    "    \n",
    "    This means that in the for loop, dimensions of the weight matrices must be (layers - l) x (layers - l - 1)\n",
    "    and for the bias matrices will be (layers - l) x 1\n",
    "    \n",
    "    ^ This looks complicated because we don't count the input layer (A0) as a layer for calculations\n",
    "    \"\"\"\n",
    "    for l in range(layers):\n",
    "        weights.append(np.random.randn(n[layers - l], n[layers - l - 1]))\n",
    "        biases.append(np.random.randn(n[layers - l], 1))\n",
    "\n",
    "    #Cost array for iterations\n",
    "    costs = []\n",
    "    # Using the same iterations as the Medium Article\n",
    "    epochs = 10000\n",
    "    #learning rate\n",
    "    alpha = 0.1\n",
    "    for i in range(epochs):\n",
    "        #reverse bias and weight list and feed into the forward prop function\n",
    "        activated_out = forward_prop(layers, weights[::-1], biases[::-1])\n",
    "        \n",
    "        #activated_out[0] is the y_hat matrix\n",
    "        costs.append(cost(activated_out[0], Y))\n",
    "        \n",
    "        dW_array, db_array = back_prop(layers, activated_out, weights)\n",
    "        for index,x in enumerate(weights):\n",
    "            weights[index] = weights[index] - alpha*dW_array[index]\n",
    "            biases[index] = biases[index] - alpha*db_array[index]\n",
    "    return costs\n",
    "\n",
    "costs = train(A0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebb8b736-f63f-4fa6-ae1e-7bd3bff1e3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "epochs = list(range(0, 10000))  # 0 to 10000 iterations\n",
    "\n",
    "# Plot\n",
    "plt.plot(epochs, costs, label='Cost Function')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cost')\n",
    "plt.title('Cost vs Epoch')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dc728fb3-77ff-4e65-93a4-d1454afaed0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function for testing data\n",
    "def test(X, Y):\n",
    "    Weights_rev = weights[::-1]\n",
    "    biases_rev = biases[::-1]\n",
    "    for l in range(layers):\n",
    "        Z =  Weights_rev[l] @ A + biases_rev[l]\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    return costs(A, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1297fdbc-79c1-4c18-a618-76d649e9058e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nI plan on coming back to this project in the future to apply standard scaling to the input data.\\nI'll probably do this next term after my probability and statistics course so that I fully understand why the process is done.\\n\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "I plan on coming back to this project in the future to apply standard scaling to the input data.\n",
    "I'll probably do this next term after my probability and statistics course so that I fully understand why the process is done.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1684ed-c483-403a-a466-fffd2e85a9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
