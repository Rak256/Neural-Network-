{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb8cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRACTICE USING AADIL'S ARTICLE - TO BE IMPLEMENTED INTO THE MINST DATABASE FOR IDENTIFYING NUMBERS\n",
    "#Establishing number of nodes in an array\n",
    "n = [2,3,3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ba8d049-24d2-48fc-a6cb-210fd18d03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Neural Network Layer Weights. If l is the current indexed layer, then the dimensions of the weight matrix should be n^[l]xn^[(l-1)] where n[l]\n",
    "#are the features in that layer\n",
    "W1 = np.random.randn(n[1], n[0])\n",
    "W2 = np.random.randn(n[2], n[1])\n",
    "W3 = np.random.randn(n[3], n[2])\n",
    "#This creates a 2 dimensional array\n",
    "\n",
    "#Neural Network bias weights. Biases are just an n[l] x 1 matrix since each node only has 1 bias.\n",
    "b1 = np.random.randn(n[1], 1)\n",
    "b2 = np.random.randn(n[2], 1)\n",
    "b3 = np.random.randn(n[3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2973da64-1bcc-4862-9871-7f9d941f9399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for layer 1 shape: (3, 2)\n",
      "Weights for layer 2 shape: (3, 3)\n",
      "Weights for layer 3 shape: (1, 3)\n",
      "bias for layer 1 shape: (3, 1)\n",
      "bias for layer 2 shape: (3, 1)\n",
      "bias for layer 3 shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#print the values as a test\n",
    "\n",
    "print(\"Weights for layer 1 shape:\", W1.shape)\n",
    "print(\"Weights for layer 2 shape:\", W2.shape)\n",
    "print(\"Weights for layer 3 shape:\", W3.shape)\n",
    "print(\"bias for layer 1 shape:\", b1.shape)\n",
    "print(\"bias for layer 2 shape:\", b2.shape)\n",
    "print(\"bias for layer 3 shape:\", b3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "529d87e1-7965-432a-86cb-50ab8db00cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "#Neural Network Training Data - Input\n",
    "X = np.array([\n",
    "    [150, 70],\n",
    "    [254, 73],\n",
    "    [312, 68],\n",
    "    [120, 60],\n",
    "    [154, 61],\n",
    "    [212, 65],\n",
    "    [216, 67],\n",
    "    [145, 67],\n",
    "    [184, 64],\n",
    "    [130, 69]\n",
    "])\n",
    "print(X.shape)\n",
    "\n",
    "#Vectorization\n",
    "A0 = X.T\n",
    "\n",
    "#confirm transpose\n",
    "print(A0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df0736b-5ccd-4acc-8859-5a93c06b4a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network - Labels\n",
    "\n",
    "y =  np.array([\n",
    "    0,  \n",
    "    1,   \n",
    "    1, \n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "m = 10\n",
    "\n",
    "#Must be in the form n[3]^m since there is only 1 binary result of cardiovacular disease for each piece of training data\n",
    "Y = y.reshape(n[3], m)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "756e6ab6-90e2-4331-9d84-796d9cc54f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the activation function\n",
    "def sigmoid(arr):\n",
    "    return 1 / (1 + np.exp(-1 * arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e894b4bf-fb59-4605-ac90-b2411ff52268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a forward propogation function\n",
    "#reversed weights and biases as parameters (list that starts with W1 and b1)\n",
    "def forward_prop(layers, Weights_rev, biases_rev, A):\n",
    "    \"\"\"\n",
    "    Rakshit's Notes:\n",
    "    Note here that I add b which is a vector to a matrix. This is not mathematically defined but in Numpy we can \"broadcast\" the column values\n",
    "     of b so that each entry of the b vector is repeatedly applied to the columns of the W @ A matrix\n",
    "     \n",
    "     You can read more by checking out this stack overflow question - https://stackoverflow.com/questions/15744402/numpy-matrix-plus-column-vector\n",
    "     And the Numpy docs - https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "    \"\"\"\n",
    "    activated_out = []\n",
    "    #Let's make this function scalable to l layers\n",
    "    for l in layers:\n",
    "        Z =  Weight_rev[l] @ A0 + biases_rev[l]\n",
    "        A = sigmoid(Z)\n",
    "        activated_out.append(A)\n",
    "    activated_out.reverse()    \n",
    "    return activated_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb41da62-5fcc-42b8-95e3-dced20cd1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = forward_prop(W1, b1, A0)\n",
    "#initial check for function working as intended\n",
    "assert A1.shape == (n[1], m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5e6ccec-6c70-40fc-9614-44bbb3af35e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2 = forward_prop(W2, b2, A1)\n",
    "A3 = forward_prop(W3, b3, A2)\n",
    "y_hat = A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5fa4404-74e9-4e3f-9a10-7c8d85f38546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat, y):\n",
    "  \"\"\"\n",
    "  y_hat should be a n^L x m matrix\n",
    "  y should be a n^L x m matrix\n",
    "  \"\"\"\n",
    "  # 1. losses is a n^L x m\n",
    "  \"\"\"\n",
    "  Rakshit's Notes:\n",
    "  Bernoulli distribution based \"binary cross-entropy loss\" calculation. Essentially we use this since we use sigmoid (which outputs the probability of cardiovascular disease) as our \n",
    "  activation function and because the Neural Network returns a true or false answer, just like a bernoulli trial.\n",
    "  \n",
    "  Therefore, we can reduce the cost by maximizing the negative of the cost function. (ECE 204 application lol)\n",
    "  \n",
    "  Also note that this is the log of the intuitive Bernoulli PDF. This is done so that we only have to deal with a summation rather than a multiplication\n",
    "  of the test samples. Since y_hat can only take values between 0 and 1 (as it is a probability), the below loss function has a minimum on this closed interval. \n",
    "  \"\"\"\n",
    "  losses = - ((y * np.log(y_hat)) + (1 - y)*np.log(1 - y_hat))\n",
    "\n",
    "  \"\"\"\n",
    "  Rakshit's Notes:\n",
    "  flattens multi-dimensional numpy array into a 1D shape array. Therefore, shape[0]\n",
    "  returns only the ammount of elements in the array.\n",
    "  \n",
    "  This is a neat trick for when the sample size is not known or if it increases later during training.\n",
    "  \"\"\"\n",
    "  m = y_hat.reshape(-1).shape[0]\n",
    "\n",
    "  # 2. summing across axis = 1 means we sum across rows, \n",
    "  #   making this a n^L x 1 matrix\n",
    "  summed_losses = (1 / m) * np.sum(losses, axis=1)\n",
    "\n",
    "  # 3. unnecessary, but useful if working with more than one node\n",
    "  #   in output layer\n",
    "  return np.sum(summed_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c690ccb5-8591-4e2b-aa78-fdcef5dbc66b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.695108249978964"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost(y_hat,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59158da-0391-4949-a4c6-1ebc1f029818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rakshit's notes - Deviating here from Aadil's code. \n",
    "# Rakshit's notes - I want to make this function scalable in case I want to add more hidden layers.\n",
    "weights = [W3,W2,W1]\n",
    "biases = [b3, b2, b1]\n",
    "activated_out = [A3, A2, A1]\n",
    "def back_prop(layers, activated_out, weights):\n",
    "    dZ = 1/m (A-Y)\n",
    "    dW_array = []\n",
    "    db_array = []\n",
    "    for x in range(layers):\n",
    "        dW = dZ @ A2.T\n",
    "        db = np.sum(dZ3, axis = 1, keepdims = True)\n",
    "        dA = weights[x].T @ dZ\n",
    "        dZ = dA*activated_out[x](1 - activated_out[x])\n",
    "        dW_array.append(dW)\n",
    "        db_array.append(db)\n",
    "    return dW_array, db_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8d1a6e2-1103-4ef9-aa17-36ab4e59d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias when initialization for layer 0 is :(1, 1)\n",
      "bias when initialization for layer 1 is :(3, 1)\n",
      "bias when initialization for layer 2 is :(3, 1)\n",
      "activated out is shaped: \n",
      "(2, 10)\n",
      "weights shape is:(3, 2)\n",
      "biases shape is:(3, 1)\n",
      "activated out is shaped: \n",
      "(3, 10)\n",
      "weights shape is:(3, 3)\n",
      "biases shape is:(3, 1)\n",
      "activated out is shaped: \n",
      "(3, 10)\n",
      "weights shape is:(1, 3)\n",
      "biases shape is:(1, 1)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36m<cell line: 166>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    163\u001b[0m             biases[index] \u001b[38;5;241m=\u001b[39m biases \u001b[38;5;241m-\u001b[39m alpha\u001b[38;5;241m*\u001b[39mdb_array[index]\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m costs\n\u001b[1;32m--> 166\u001b[0m costs \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA0\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28mprint\u001b[39m(costs)\n",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(A0)\u001b[0m\n\u001b[0;32m    158\u001b[0m activated_out \u001b[38;5;241m=\u001b[39m forward_prop(layers, weights[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], biases[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    159\u001b[0m costs\u001b[38;5;241m.\u001b[39mappend(cost(activated_out[\u001b[38;5;241m0\u001b[39m], Y))\n\u001b[1;32m--> 160\u001b[0m dW_array, db_array \u001b[38;5;241m=\u001b[39m \u001b[43mback_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivated_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index,x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(weights):\n\u001b[0;32m    162\u001b[0m     weights[index] \u001b[38;5;241m=\u001b[39m weights \u001b[38;5;241m-\u001b[39m alpha\u001b[38;5;241m*\u001b[39mdW_array[index]\n",
      "Input \u001b[1;32mIn [50]\u001b[0m, in \u001b[0;36mback_prop\u001b[1;34m(layers, activated_out, weights)\u001b[0m\n\u001b[0;32m    125\u001b[0m dW \u001b[38;5;241m=\u001b[39m dZ \u001b[38;5;241m@\u001b[39m activated_out[l \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mT\n\u001b[0;32m    126\u001b[0m db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dZ, axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, keepdims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 127\u001b[0m dA \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdZ\u001b[49m\n\u001b[0;32m    128\u001b[0m dZ \u001b[38;5;241m=\u001b[39m dA\u001b[38;5;241m*\u001b[39mactivated_out[l]\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m activated_out[l])\n\u001b[0;32m    129\u001b[0m dW_array\u001b[38;5;241m.\u001b[39mappend(dW)\n",
      "\u001b[1;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 3 is different from 1)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Rakshit's Notes:\n",
    "Final setup - Putting everything together.\n",
    "Here I create a train function similar to Aadil's Medium article\n",
    "all the comments below are mine\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "#These variables would have to be used with the test function and the main program function\n",
    "# So, it's better to make them global\n",
    "#Node layout\n",
    "n = [2,3,3,1]\n",
    "#Neural Network Training Data - Input\n",
    "X = np.array([\n",
    "    [150, 70],\n",
    "    [254, 73],\n",
    "    [312, 68],\n",
    "    [120, 60],\n",
    "    [154, 61],\n",
    "    [212, 65],\n",
    "    [216, 67],\n",
    "    [145, 67],\n",
    "    [184, 64],\n",
    "    [130, 69]\n",
    "])\n",
    "\n",
    "#Vectorization\n",
    "A0 = X.T\n",
    "\n",
    "#Neural Network - Labels\n",
    "\n",
    "y =  np.array([\n",
    "    0,  \n",
    "    1,   \n",
    "    1, \n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "m = 10\n",
    "\n",
    "#Must be in the form n[3]^m since there is only 1 binary result of cardiovacular disease for each piece of training data\n",
    "Y = y.reshape(n[3], m)\n",
    "\n",
    "\n",
    "weights = [] \n",
    "biases = []\n",
    "\n",
    "#define the activation function\n",
    "def sigmoid(arr):\n",
    "    return 1 / (1 + np.exp(-1 * arr))\n",
    "\n",
    "#create a forward propogation function\n",
    "#reversed weights and biases as parameters (list that starts with W1 and b1)\n",
    "def forward_prop(layers, Weights_rev, biases_rev):\n",
    "    \"\"\"\n",
    "    Rakshit's Notes:\n",
    "    Note here that I add b which is a vector to a matrix. This is not mathematically defined but in Numpy we can \"broadcast\" the column values\n",
    "     of b so that each entry of the b vector is repeatedly applied to the columns of the W @ A matrix\n",
    "     \n",
    "     You can read more by checking out this stack overflow question - https://stackoverflow.com/questions/15744402/numpy-matrix-plus-column-vector\n",
    "     And the Numpy docs - https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "    \"\"\"\n",
    "    activated_out = []\n",
    "    #Let's make this function scalable to l layers\n",
    "    A = A0\n",
    "    for l in range(layers):\n",
    "        activated_out.append(A)\n",
    "        print(\"activated out is shaped: \")\n",
    "        print(activated_out[l].shape)\n",
    "        print(\"weights shape is:\" + str(Weights_rev[l].shape))\n",
    "        print(\"biases shape is:\" + str(biases_rev[l].shape)) \n",
    "        Z =  Weights_rev[l] @ A + biases_rev[l]\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    activated_out.reverse()    \n",
    "    return activated_out\n",
    "\n",
    "def cost(y_hat, y):\n",
    "  \"\"\"\n",
    "  y_hat should be a n^L x m matrix\n",
    "  y should be a n^L x m matrix\n",
    "  \"\"\"\n",
    "  # 1. losses is a n^L x m\n",
    "  \"\"\"\n",
    "  Rakshit's Notes:\n",
    "  Bernoulli distribution based \"binary cross-entropy loss\" calculation. Essentially we use this since we use sigmoid (which outputs the probability of cardiovascular disease) as our \n",
    "  activation function and because the Neural Network returns a true or false answer, just like a bernoulli trial.\n",
    "  \n",
    "  Therefore, we can reduce the cost by maximizing the negative of the cost function. (ECE 204 application lol)\n",
    "  \n",
    "  Also note that this is the log of the intuitive Bernoulli PDF. This is done so that we only have to deal with a summation rather than a multiplication\n",
    "  of the test samples. Since y_hat can only take values between 0 and 1 (as it is a probability), the below loss function has a minimum on this closed interval. \n",
    "  \"\"\"\n",
    "  losses = - ((y * np.log(y_hat)) + (1 - y)*np.log(1 - y_hat))\n",
    "\n",
    "  \"\"\"\n",
    "  Rakshit's Notes:\n",
    "  flattens multi-dimensional numpy array into a 1D shape array. Therefore, shape[0]\n",
    "  returns only the ammount of elements in the array.\n",
    "  \n",
    "  This is a neat trick for when the sample size is not known or if it increases later during training.\n",
    "  \"\"\"\n",
    "  m = y_hat.reshape(-1).shape[0]\n",
    "\n",
    "  # 2. summing across axis = 1 means we sum across rows, \n",
    "  #   making this a n^L x 1 matrix\n",
    "  summed_losses = (1 / m) * np.sum(losses, axis=1)\n",
    "\n",
    "  # 3. unnecessary, but useful if working with more than one node\n",
    "  #   in output layer\n",
    "  return np.sum(summed_losses)\n",
    "\n",
    "# Rakshit's notes - Deviating here from Aadil's code. \n",
    "# Rakshit's notes - I want to make this function scalable in case I want to add more hidden layers.\n",
    "def back_prop(layers, activated_out, weights):\n",
    "    dZ = (1/m)*(activated_out[0] - Y)\n",
    "    dW_array = []\n",
    "    db_array = []\n",
    "    for l in range(layers):\n",
    "        dW = dZ @ activated_out[l + 1].T\n",
    "        db = np.sum(dZ, axis = 1, keepdims = True)\n",
    "        dA = (weights[l].T) @ dZ\n",
    "        dZ = dA*activated_out[l]*(1 - activated_out[l])\n",
    "        dW_array.append(dW)\n",
    "        db_array.append(db)\n",
    "    return dW_array, db_array\n",
    "\n",
    "def train(A0):\n",
    "    layers = 3\n",
    "    \n",
    "    \"\"\"\n",
    "    Goal: To create weight matrix and bias matrix, indexed starting from the\n",
    "    outer layers of the network.\n",
    "    \n",
    "    This means that in the for loop, dimensions of the weight matrices must be (layers - l) x (layers - l - 1)\n",
    "    and for the bias matrices will be (layers - l) x 1\n",
    "    \n",
    "    ^ This looks complicated because we don't count the input layers as a layer for calculations\n",
    "    \"\"\"\n",
    "    for l in range(layers):\n",
    "        weights.append(np.random.randn(n[layers - l], n[layers - l - 1]))\n",
    "        biases.append(np.random.randn(n[layers - l], 1))\n",
    "        print (\"bias when initialization for layer \" + str(l) + \" is :\" + str(biases[l].shape))\n",
    "\n",
    "    #Cost array for iterations\n",
    "    costs = []\n",
    "    # Using the same iterations as the Medium Article\n",
    "    epochs = 10000\n",
    "    #learning rate\n",
    "    alpha = 0.01\n",
    "    for i in range(epochs):\n",
    "        #reverse bias and weight list and feed into the forward prop function\n",
    "        activated_out = forward_prop(layers, weights[::-1], biases[::-1])\n",
    "        costs.append(cost(activated_out[0], Y))\n",
    "        dW_array, db_array = back_prop(layers, activated_out, weights)\n",
    "        for index,x in enumerate(weights):\n",
    "            weights[index] = weights - alpha*dW_array[index]\n",
    "            biases[index] = biases - alpha*db_array[index]\n",
    "    return costs\n",
    "\n",
    "costs = train(A0)\n",
    "\n",
    "print(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb8b736-f63f-4fa6-ae1e-7bd3bff1e3a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc728fb3-77ff-4e65-93a4-d1454afaed0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
