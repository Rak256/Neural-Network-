{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "aeb8cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PRACTICE USING AADIL'S ARTICLE - TO BE IMPLEMENTED INTO THE MINST DATABASE FOR IDENTIFYING NUMBERS\n",
    "#Establishing number of nodes in an array\n",
    "n = [2,3,3,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ba8d049-24d2-48fc-a6cb-210fd18d03cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#Neural Network Layer Weights. If l is the current indexed layer, then the dimensions of the weight matrix should be n^[l]xn^[(l-1)] where n[l]\n",
    "#are the features in that layer\n",
    "W1 = np.random.randn(n[1], n[0])\n",
    "W2 = np.random.randn(n[2], n[1])\n",
    "W3 = np.random.randn(n[3], n[2])\n",
    "#This creates a 2 dimensional array\n",
    "\n",
    "#Neural Network bias weights. Biases are just an n[l] x 1 matrix since each node only has 1 bias.\n",
    "b1 = np.random.randn(n[1], 1)\n",
    "b2 = np.random.randn(n[2], 1)\n",
    "b3 = np.random.randn(n[3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2973da64-1bcc-4862-9871-7f9d941f9399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights for layer 1 shape: (3, 2)\n",
      "Weights for layer 2 shape: (3, 3)\n",
      "Weights for layer 3 shape: (1, 3)\n",
      "bias for layer 1 shape: (3, 1)\n",
      "bias for layer 2 shape: (3, 1)\n",
      "bias for layer 3 shape: (1, 1)\n"
     ]
    }
   ],
   "source": [
    "#print the values as a test\n",
    "\n",
    "print(\"Weights for layer 1 shape:\", W1.shape)\n",
    "print(\"Weights for layer 2 shape:\", W2.shape)\n",
    "print(\"Weights for layer 3 shape:\", W3.shape)\n",
    "print(\"bias for layer 1 shape:\", b1.shape)\n",
    "print(\"bias for layer 2 shape:\", b2.shape)\n",
    "print(\"bias for layer 3 shape:\", b3.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "529d87e1-7965-432a-86cb-50ab8db00cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 2)\n",
      "(2, 10)\n"
     ]
    }
   ],
   "source": [
    "#Neural Network Training Data - Input\n",
    "X = np.array([\n",
    "    [150, 70],\n",
    "    [254, 73],\n",
    "    [312, 68],\n",
    "    [120, 60],\n",
    "    [154, 61],\n",
    "    [212, 65],\n",
    "    [216, 67],\n",
    "    [145, 67],\n",
    "    [184, 64],\n",
    "    [130, 69]\n",
    "])\n",
    "print(X.shape)\n",
    "\n",
    "#Vectorization\n",
    "A0 = X.T\n",
    "\n",
    "#confirm transpose\n",
    "print(A0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "2df0736b-5ccd-4acc-8859-5a93c06b4a75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 10)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Neural Network - Labels\n",
    "\n",
    "y =  np.array([\n",
    "    0,  \n",
    "    1,   \n",
    "    1, \n",
    "    0,\n",
    "    0,\n",
    "    1,\n",
    "    1,\n",
    "    0,\n",
    "    1,\n",
    "    0\n",
    "])\n",
    "\n",
    "m = 10\n",
    "\n",
    "#Must be in the form n[3]^m since there is only 1 binary result of cardiovacular disease for each piece of training data\n",
    "Y = y.reshape(n[3], m)\n",
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "756e6ab6-90e2-4331-9d84-796d9cc54f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the activation function\n",
    "def sigmoid(arr):\n",
    "    return 1 / (1 + np.exp(-1 * arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e894b4bf-fb59-4605-ac90-b2411ff52268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a forward propogation function\n",
    "def forward_prop(W, b, A):\n",
    "    \"\"\"\n",
    "    Rakshit's Notes:\n",
    "    Note here that I add b which is a vector to a matrix. This is not mathematically defined but in Numpy we can \"broadcast\" the column values\n",
    "     of b so that each entry of the b vector is repeatedly applied to the columns of the W @ A matrix\n",
    "     \n",
    "     You can read more by checking out this stack overflow question - https://stackoverflow.com/questions/15744402/numpy-matrix-plus-column-vector\n",
    "     And the Numpy docs - https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "    \"\"\" \n",
    "    Z =  W @ A + b\n",
    "    return sigmoid(Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "cb41da62-5fcc-42b8-95e3-dced20cd1c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = forward_prop(W1, b1, A0)\n",
    "#initial check for function working as intended\n",
    "assert A1.shape == (n[1], m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f5e6ccec-6c70-40fc-9614-44bbb3af35e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88636048, 0.88636048, 0.88636048, 0.88636048, 0.88636048,\n",
       "        0.88636048, 0.88636048, 0.88636048, 0.88636048, 0.88636048]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A2 = forward_prop(W2, b2, A1)\n",
    "A3 = forward_prop(W3, b3, A2)\n",
    "A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5fa4404-74e9-4e3f-9a10-7c8d85f38546",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(y_hat, y):\n",
    "  \"\"\"\n",
    "  y_hat should be a n^L x m matrix\n",
    "  y should be a n^L x m matrix\n",
    "  \"\"\"\n",
    "  # 1. losses is a n^L x m\n",
    "  \"\"\"\n",
    "  Rakshit's Notes:\n",
    "  Bernoulli distribution based \"binary cross-entropy loss\" calculation. Essentially we use this since we use sigmoid (which outputs the probability of cardiovascular disease) as our \n",
    "  activation function and because the Neural Network returns a true or false answer, just like a bernoulli trial.\n",
    "  \n",
    "  Therefore, we can reduce the cost by maximizing the negative of the cost function. (ECE 204 application lol)\n",
    "  \n",
    "  Also note that this is the log of the intuitive Bernoulli PDF. This is done so that we only have to deal with a summation rather than a multiplication\n",
    "  of the test samples. And, as you may know from calculus courses, \n",
    "  \"\"\"\n",
    "  losses = - ( (y * np.log(y_hat)) + (1 - y)*np.log(1 - y_hat) )\n",
    "\n",
    "  m = y_hat.reshape(-1).shape[0]\n",
    "\n",
    "  # 2. summing across axis = 1 means we sum across rows, \n",
    "  #   making this a n^L x 1 matrix\n",
    "  summed_losses = (1 / m) * np.sum(losses, axis=1)\n",
    "\n",
    "  # 3. unnecessary, but useful if working with more than one node\n",
    "  #   in output layer\n",
    "  return np.sum(summed_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c690ccb5-8591-4e2b-aa78-fdcef5dbc66b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
